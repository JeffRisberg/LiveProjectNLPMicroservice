{
 "cells": [],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n",
     "import nltk\n",
     "nltk.download('wordnet')\n",
     "\n",
     "from nltk import data\n",
     "from nltk.corpus import stopwords\n",
     "from string import punctuation\n",
     "from nltk.stem import WordNetLemmatizer\n",
     "import re\n",
     "from nltk.util import everygrams\n",
     "import pickle\n",
     "from nltk.tokenize import word_tokenize\n",
     "\n",
     "#from nltk import download\n",
     "download('punkt',download_dir=data.path[0])\n",
     "download('stopwords',download_dir=data.path[0])\n",
     "download('wordnet',download_dir=data.path[0])\n",
     "# #THEN remove the zip files!\n",
     "\n",
     "data.path=['nltk_data']\n",
     "stopwords_eng = stopwords.words(\"english\")\n",
     "\n",
     "lemmatizer = WordNetLemmatizer()\n",
     "\n",
     "def extract_features(document):\n",
     "    words = word_tokenize(document)\n",
     "    lemmas = [str(lemmatizer.lemmatize(w)) for w in words if w not in stopwords_eng and w not in punctuation]\n",
     "    document = \" \".join(lemmas)\n",
     "    document = document.lower()\n",
     "    document = re.sub(r'[^a-zA-Z0-9\\s]', ' ', document)\n",
     "    words = [w for w in document.split(\" \") if w!=\"\" and w not in stopwords_eng and w not in punctuation]\n",
     "    return [str('_'.join(ngram)) for ngram in list(everygrams(words, max_len=3))]\n",
     "\n",
     "print(extract_features(\"Hello world, corpuses calling!\"))\n",
     "\n",
     "def bag_of_words(words):\n",
     "    bag = {}\n",
     "    for w in words:\n",
     "        bag[w] = bag.get(w,0)+1\n",
     "    return bag\n",
     "\n",
     "\n",
     "import pickle\n",
     "import sys\n",
     "\n",
     "if not 'google.colab' in sys.modules:\n",
     "    model_file = open('sa_classifier.pickle', 'rb')\n",
     "    model = pickle.load(model_file)\n",
     "    model_file.close()\n",
     "\n",
     "from nltk.tokenize import word_tokenize\n",
     "\n",
     "def get_sentiment(review):\n",
     "    #words = word_tokenize(review)\n",
     "    words = extract_features(review)\n",
     "    words = bag_of_words(words)\n",
     "    return model.classify(words)\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}